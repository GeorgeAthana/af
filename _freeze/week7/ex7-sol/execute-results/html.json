{
  "hash": "dd6268234ee88efd8b3b6ce7a65bc8de",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Exercise Week 7: Solutions\"\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fpp3)\n```\n:::\n\n\n\n\n# fpp3 8.8, Ex5\n\n>  Data set `global_economy` contains the annual Exports from many countries. Select one country to analyse.\n>\n>    a. Plot the Exports series and discuss the main features of the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_economy |>\n  filter(Country == \"Argentina\") |>\n  autoplot(Exports)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nThere is a huge jump in Exports in 2002, due to the deregulation of the Argentinian peso. Since then, Exports (as a percentage of GDP) has gradually returned to 1990 levels.\n\n>    b. Use an ETS(A,N,N) model to forecast the series, and plot the forecasts.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\netsANN <- global_economy |>\n  filter(Country == \"Argentina\") |>\n  model(ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")))\netsANN |>\n  forecast(h = 10) |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n>    c. Compute the RMSE values for the training data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(etsANN) |> select(RMSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n   RMSE\n  <dbl>\n1  2.78\n```\n\n\n:::\n:::\n\n\n\n\n>    d. Compare the results to those from an ETS(A,A,N) model. (Remember that the trended model is using one more parameter than the simpler model.) Discuss the merits of the two forecasting methods for this data set.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- global_economy |>\n  filter(Country == \"Argentina\") |>\n  model(\n    ses = ETS(Exports ~ error(\"A\") + trend(\"N\") + season(\"N\")),\n    holt = ETS(Exports ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n  )\naccuracy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 11\n  Country   .model .type         ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  <fct>     <chr>  <chr>      <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 Argentina ses    Training 0.0762   2.78  1.62 -1.73  15.7 0.983 0.986 0.00902\n2 Argentina holt   Training 0.00795  2.78  1.64 -2.51  15.9 0.994 0.986 0.0271 \n```\n\n\n:::\n:::\n\n\n\n\nThere is very little difference in training RMSE between these models. So the extra parameter is not doing much.\n\n>    e. Compare the forecasts from both methods. Which do you think is best?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 10) |>\n  autoplot(global_economy)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\nThe forecasts are similar. In this case, the simpler model is preferred.\n\n>    f. Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using R.\n\n1.  standard error. (from RMSE)\n2.  mean (from forecast)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns <- accuracy(fit) |> pull(RMSE)\nyhat <- forecast(fit, h = 1) |> pull(.mean)\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.882074 16.764136\n```\n\n\n:::\n\n```{.r .cell-code}\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.989515 16.872908\n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 1) |>\n  mutate(PI = hilo(Exports, level = 95))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A fable: 2 x 6 [1Y]\n# Key:     Country, .model [2]\n  Country   .model  Year    Exports .mean                     PI\n  <fct>     <chr>  <dbl>     <dist> <dbl>                 <hilo>\n1 Argentina ses     2018   N(11, 8)  11.3 [5.785765, 16.86044]95\n2 Argentina holt    2018 N(11, 8.3)  11.4 [5.791571, 17.07085]95\n```\n\n\n:::\n:::\n\n\n\n\n-   Using RMSE yields narrower prediction interval while using the values from\n    `hilo()` function gives wider prediction interval.\n\n-   Using RMSE has failed to take account of the degrees of freedom for each\n    model. Compare the following\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsse <- augment(fit) |>\n  as_tibble() |>\n  group_by(.model) |>\n  summarise(s = sum(.resid^2)) |>\n  pull(s)\n\nn <- global_economy |>\n  filter(Country == \"Argentina\") |>\n    nrow()\n\n# sse method= alpha, level=> 2\n# holt linear = alpha, level, trend, b => 4\n\ns <- sqrt(sse / (n - c(2, 4)))\n\n# SES\nyhat[1] + c(-1, 1) * qnorm(0.975) * s[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.785088 16.861122\n```\n\n\n:::\n\n```{.r .cell-code}\n# Holt\nyhat[2] + c(-1, 1) * qnorm(0.975) * s[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  5.79226 17.07016\n```\n\n\n:::\n:::\n\n\n\n\n# fpp3 8.8, Ex6\n\n> Forecast the Chinese GDP from the `global_economy` data set using an ETS model. Experiment with the various options in the `ETS()` function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each is doing to the forecasts.\n\n> [Hint: use `h=20` when forecasting, so you can clearly see the differences between the various options when plotting the forecasts.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchina <- global_economy |>\n  filter(Country == \"China\")\nchina |> autoplot(GDP)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n\n-   It clearly needs a relatively strong transformation due to the increasing\n    variance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchina |> autoplot(box_cox(GDP, 0.2))\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n\n```{.r .cell-code}\nchina |> features(GDP, guerrero)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  Country lambda_guerrero\n  <fct>             <dbl>\n1 China           -0.0345\n```\n\n\n:::\n:::\n\n\n\n\n-   Making $\\lambda=0.2$ looks ok.\n\n-   The Guerrero method suggests an even stronger transformation. Let's also try\n    a log.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- china |>\n  model(\n    ets = ETS(GDP),\n    ets_damped = ETS(GDP ~ trend(\"Ad\")),\n    ets_bc = ETS(box_cox(GDP, 0.2)),\n    ets_log = ETS(log(GDP))\n  )\n\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 5\n# Key:     Country [1]\n  Country          ets    ets_damped       ets_bc      ets_log\n  <fct>        <model>       <model>      <model>      <model>\n1 China   <ETS(M,A,N)> <ETS(M,Ad,N)> <ETS(A,A,N)> <ETS(A,A,N)>\n```\n\n\n:::\n\n```{.r .cell-code}\naugment(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tsibble: 232 x 7 [1Y]\n# Key:       Country, .model [4]\n   Country .model  Year          GDP      .fitted        .resid   .innov\n   <fct>   <chr>  <dbl>        <dbl>        <dbl>         <dbl>    <dbl>\n 1 China   ets     1960 59716467625. 49001691297.  10714776328.  0.219  \n 2 China   ets     1961 50056868958. 66346643194. -16289774236. -0.246  \n 3 China   ets     1962 47209359006. 51607368186.  -4398009180. -0.0852 \n 4 China   ets     1963 50706799903. 47386494407.   3320305495.  0.0701 \n 5 China   ets     1964 59708343489. 51919091574.   7789251914.  0.150  \n 6 China   ets     1965 70436266147. 63350421234.   7085844913.  0.112  \n 7 China   ets     1966 76720285970. 76289186599.    431099371.  0.00565\n 8 China   ets     1967 72881631327. 82708375812.  -9826744486. -0.119  \n 9 China   ets     1968 70846535056. 75804820984.  -4958285928. -0.0654 \n10 China   ets     1969 79705906247. 72222259470.   7483646777.  0.104  \n# ℹ 222 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n  forecast(h = \"20 years\") |>\n  autoplot(china, level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\n-   The transformations have a big effect, with small lambda values creating big\n    increases in the forecasts.\n-   The damping has relatively a small effect.\n\n# fpp3 8.8, Ex7\n\n> Find an ETS model for the Gas data from `aus_production` and forecast the next few years. Why is multiplicative seasonality necessary here? Experiment with making the trend damped. Does it improve the forecasts?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_production |> autoplot(Gas)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n-   There is a huge increase in variance as the series increases in level. =\\>\n    That makes it necessary to use multiplicative seasonality.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_production |>\n  model(\n    hw = ETS(Gas ~ error(\"M\") + trend(\"A\") + season(\"M\")),\n    hwdamped = ETS(Gas ~ error(\"M\") + trend(\"Ad\") + season(\"M\")),\n  )\n\nfit |> glance()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n  .model    sigma2 log_lik   AIC  AICc   BIC   MSE  AMSE    MAE\n  <chr>      <dbl>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>\n1 hw       0.00324   -831. 1681. 1682. 1711.  21.1  32.2 0.0413\n2 hwdamped 0.00329   -832. 1684. 1685. 1718.  21.1  32.0 0.0417\n```\n\n\n:::\n:::\n\n\n\n\n- The non-damped model seems to be doing slightly better here, probably because the trend is very strong over most of the historical data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(hw) |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 19 × 3\n   .model   term  estimate\n   <chr>    <chr>    <dbl>\n 1 hw       alpha   0.653 \n 2 hw       beta    0.144 \n 3 hw       gamma   0.0978\n 4 hw       l[0]    5.95  \n 5 hw       b[0]    0.0706\n 6 hw       s[0]    0.931 \n 7 hw       s[-1]   1.18  \n 8 hw       s[-2]   1.07  \n 9 hw       s[-3]   0.816 \n10 hwdamped alpha   0.649 \n11 hwdamped beta    0.155 \n12 hwdamped gamma   0.0937\n13 hwdamped phi     0.980 \n14 hwdamped l[0]    5.86  \n15 hwdamped b[0]    0.0994\n16 hwdamped s[0]    0.928 \n17 hwdamped s[-1]   1.18  \n18 hwdamped s[-2]   1.08  \n19 hwdamped s[-3]   0.817 \n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n  augment() |>\n  filter(.model == \"hw\") |>\n  features(.innov, ljung_box, lag = 24)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .model lb_stat lb_pvalue\n  <chr>    <dbl>     <dbl>\n1 hw        57.1  0.000161\n```\n\n\n:::\n:::\n\n\n\n\n- There is still some small *correlations* left in the residuals, showing the model has not fully captured the available information.\n- There also appears to be some *heteroskedasticity* in the residuals with larger variance in the first half the series.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 36) |>\n  filter(.model == \"hw\") |>\n  autoplot(aus_production)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\n\nWhile the point forecasts look ok, the intervals are excessively wide.\n\n# fpp3 8.8, Ex8\n\n> Recall your retail time series data (from Exercise 6 in Section 2.10).\n>\n>    a. Why is multiplicative seasonality necessary for this series?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345678)\nmyseries <- aus_retail |>\n  filter(`Series ID` == sample(aus_retail$`Series ID`, 1))\nmyseries |> autoplot(Turnover)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/89a-1.png){width=672}\n:::\n:::\n\n\n\n\nThe variation in the seasonal pattern increases as the level of the series rises. (This may not be true for every series, but is true for almost all of them.)\n\n>    b. Apply Holt-Winters’ multiplicative method to the data. Experiment with making the trend damped.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- myseries |>\n  model(\n    hw = ETS(Turnover ~ error(\"M\") + trend(\"A\") + season(\"M\")),\n    hwdamped = ETS(Turnover ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n  )\nfc <- fit |> forecast(h = 36)\nfc |> autoplot(myseries)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/89b-1.png){width=672}\n:::\n:::\n\n\n\n\n>    c. Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naccuracy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 12\n  State      Industry .model .type      ME  RMSE   MAE     MPE  MAPE  MASE RMSSE\n  <chr>      <chr>    <chr>  <chr>   <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 Northern … Clothin… hw     Trai… -0.0128 0.613 0.450 -0.469   5.15 0.513 0.529\n2 Northern … Clothin… hwdam… Trai…  0.0398 0.616 0.444 -0.0723  5.06 0.507 0.531\n# ℹ 1 more variable: ACF1 <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nThe non-damped method is doing slightly better (on RMSE), but the damped method is doing better on most other scores. I'd be inclined to use the damped method here as the trend at the end of the series seems to be flattening.\n\n>    d. Check that the residuals from the best method look like white noise.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  select(\"hwdamped\") |>\n  gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/89d-1.png){width=672}\n:::\n:::\n\n\n\n\nThere are significant spikes at lags 8 and 18 in the ACF, but they are relatively small and probably of no consequence.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(fit) |>\n  filter(.model == \"hwdamped\") |>\n  features(.innov, ljung_box, lag = 36)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n  State              Industry                           .model lb_stat lb_pvalue\n  <chr>              <chr>                              <chr>    <dbl>     <dbl>\n1 Northern Territory Clothing, footwear and personal a… hwdam…    35.2     0.507\n```\n\n\n:::\n:::\n\n\n\n\nIt seems that there is enough autocorrelation in the residuals for this to be significant. But in practice it will make little difference to the forecasts or prediction intervals.\n\n>    e. Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naïve approach from Exercise 7 in Section 5.10?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmyseries |>\n  filter(year(Month) < 2011) |>\n  model(\n    snaive = SNAIVE(Turnover),\n    hw = ETS(Turnover ~ error(\"M\") + trend(\"A\") + season(\"M\")),\n    hwdamped = ETS(Turnover ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n  ) |>\n  forecast(h = \"7 years\") |>\n  accuracy(myseries)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 12\n  .model   State    Industry .type      ME  RMSE   MAE     MPE  MAPE  MASE RMSSE\n  <chr>    <chr>    <chr>    <chr>   <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 hw       Norther… Clothin… Test  -1.45    1.71 1.51  -11.8   12.2  1.66  1.41 \n2 hwdamped Norther… Clothin… Test   0.0290  1.13 0.846  -0.831  6.32 0.925 0.934\n3 snaive   Norther… Clothin… Test   0.673   1.45 1.13    4.77   8.34 1.24  1.19 \n# ℹ 1 more variable: ACF1 <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nThe SNAIVE model is doing much better than the HW model for this data set.\n\n# fpp3 8.8, Ex9\n\n> For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstl <- decomposition_model(\n  STL(log(Turnover)),\n  ETS(season_adjust ~ season(\"N\"))\n)\nfc <- myseries |>\n  filter(year(Month) < 2011) |>\n  model(\n    stl_ets = stl,\n    snaive = SNAIVE(Turnover),\n    hwdamped = ETS(Turnover ~ error(\"M\") + trend(\"Ad\") + season(\"M\"))\n  ) |>\n  forecast(h = \"7 years\")\nfc |> autoplot(filter(myseries, year(Month) > 2004), level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/810-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |> accuracy(myseries)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 12\n  .model   State    Industry .type      ME  RMSE   MAE     MPE  MAPE  MASE RMSSE\n  <chr>    <chr>    <chr>    <chr>   <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl> <dbl>\n1 hwdamped Norther… Clothin… Test   0.0290  1.13 0.846  -0.831  6.32 0.925 0.934\n2 snaive   Norther… Clothin… Test   0.673   1.45 1.13    4.77   8.34 1.24  1.19 \n3 stl_ets  Norther… Clothin… Test  -4.28    4.86 4.28  -32.7   32.7  4.68  4.01 \n# ℹ 1 more variable: ACF1 <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nThe data since 2011 has been less trended than what was seen before that. So the models with the lowest trend have done better. The STL+ETS approach has the highest trends and therefore the worst accuracy.\n\n# fpp3 8.8, Ex10\n\n> Compute the total domestic overnight trips for holidays across Australia from the `tourism` dataset.\n\n>   a. Plot the data and describe the main features of the series.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_trips <- tourism |>\n  summarise(Trips = sum(Trips))\naus_trips |>\n  autoplot(Trips)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\n\n- The data is seasonal.\n- A slightly decreasing trend exists until 2010, after which it is replaced with a stronger upward trend.\n\n>   b. Decompose the series using STL and obtain the seasonally adjusted data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndcmp <- aus_trips |>\n  model(STL(Trips)) |>\n  components()\ndcmp |>\n  as_tsibble() |>\n  autoplot(season_adjust)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n\n\n>   c. Forecast the next two years of the series using an additive damped trend method applied to the seasonally adjusted data. (This can be specified using `decomposition_model()`.)\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstletsdamped <- decomposition_model(\n  STL(Trips),\n  ETS(season_adjust ~ error(\"A\") + trend(\"Ad\") + season(\"N\"))\n)\naus_trips |>\n  model(dcmp_AAdN = stletsdamped) |>\n  forecast(h = \"2 years\") |>\n  autoplot(aus_trips)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n\n>   d. Forecast the next two years of the series using an appropriate model for Holt’s linear method applied to the seasonally adjusted data (as before but without damped trend).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstletstrend <- decomposition_model(\n  STL(Trips),\n  ETS(season_adjust ~ error(\"A\") + trend(\"A\") + season(\"N\"))\n)\naus_trips |>\n  model(dcmp_AAN = stletstrend) |>\n  forecast(h = \"2 years\") |>\n  autoplot(aus_trips)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n>   e. Now use `ETS()` to choose a seasonal model for the data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_trips |>\n        model(ets = ETS(Trips))\n\nfit |> report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Trips \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4495675 \n    beta  = 0.04450178 \n    gamma = 0.0001000075 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]     s[-2]    s[-3]\n 21689.64 -58.46946 -125.8548 -816.3416 -324.5553 1266.752\n\n  sigma^2:  699901.4\n\n     AIC     AICc      BIC \n1436.829 1439.400 1458.267 \n```\n\n\n:::\n\n```{.r .cell-code}\nfit |> tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  .model term      estimate\n  <chr>  <chr>        <dbl>\n1 ets    alpha     0.450   \n2 ets    beta      0.0445  \n3 ets    gamma     0.000100\n4 ets    l[0]  21690.      \n5 ets    b[0]    -58.5     \n6 ets    s[0]   -126.      \n7 ets    s[-1]  -816.      \n8 ets    s[-2]  -325.      \n9 ets    s[-3]  1267.      \n```\n\n\n:::\n\n```{.r .cell-code}\nfit |> forecast(h = \"2 years\") |>\n  autoplot(aus_trips)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n>   f. Compare the RMSE of the ETS model with the RMSE of the models you obtained using STL decompositions. Which gives the better in-sample fits?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- aus_trips |>\n  model(\n    dcmp_AAdN = stletsdamped,\n    dcmp_AAN = stletstrend,\n    ets = ETS(Trips)\n  )\n\nfit |>\n    select(ets) |>\n    tidy()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 3\n  .model term      estimate\n  <chr>  <chr>        <dbl>\n1 ets    alpha     0.450   \n2 ets    beta      0.0445  \n3 ets    gamma     0.000100\n4 ets    l[0]  21690.      \n5 ets    b[0]    -58.5     \n6 ets    s[0]   -126.      \n7 ets    s[-1]  -816.      \n8 ets    s[-2]  -325.      \n9 ets    s[-3]  1267.      \n```\n\n\n:::\n\n```{.r .cell-code}\nfit |>\n    select(ets) |>\n    report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Trips \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4495675 \n    beta  = 0.04450178 \n    gamma = 0.0001000075 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]     s[-2]    s[-3]\n 21689.64 -58.46946 -125.8548 -816.3416 -324.5553 1266.752\n\n  sigma^2:  699901.4\n\n     AIC     AICc      BIC \n1436.829 1439.400 1458.267 \n```\n\n\n:::\n\n```{.r .cell-code}\naccuracy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model    .type       ME  RMSE   MAE   MPE  MAPE  MASE RMSSE     ACF1\n  <chr>     <chr>    <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>\n1 dcmp_AAdN Training 103.   763.  576. 0.367  2.72 0.607 0.629 -0.0174 \n2 dcmp_AAN  Training  99.7  763.  574. 0.359  2.71 0.604 0.628 -0.0182 \n3 ets       Training 105.   794.  604. 0.379  2.86 0.636 0.653 -0.00151\n```\n\n\n:::\n:::\n\n\n\n\n- The STL decomposition forecasts using the additive trend model, ETS(A,A,N), is slightly better in-sample.\n- However, note that this is a biased comparison as the models have different numbers of parameters.\n\n>   g. Compare the forecasts from the three approaches? Which seems most reasonable?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = \"2 years\") |>\n  autoplot(aus_trips, level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n\nThe forecasts are almost identical. So I'll use the decomposition model with additive trend as it has the smallest RMSE.\n\n>   h. Check the residuals of your preferred model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbest <- fit |>\n  select(dcmp_AAN)\n\nreport(best)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Trips \nModel: STL decomposition model \nCombination: season_adjust + season_year\n\n========================================\n\nSeries: season_adjust \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.4806411 \n    beta  = 0.04529462 \n\n  Initial states:\n     l[0]     b[0]\n 21366.07 1.279692\n\n  sigma^2:  585061\n\n     AIC     AICc      BIC \n1418.816 1419.627 1430.727 \n\nSeries: season_year \nModel: SNAIVE \n\nsigma^2: 3210.8552 \n```\n\n\n:::\n\n```{.r .cell-code}\naugment(best) |> gg_tsdisplay(.resid, lag_max = 24, plot_type = \"histogram\")\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(best) |> features(.innov, ljung_box, lag = 24)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n  .model   lb_stat lb_pvalue\n  <chr>      <dbl>     <dbl>\n1 dcmp_AAN    33.3    0.0976\n```\n\n\n:::\n:::\n\n\n\n\n- The residuals look okay however there still remains some significant auto-correlation. Nevertheless, the results pass the Ljung-Box test.\n- The large spike at lag 14 can probably be ignored.\n\n# fpp3 8.8, Ex11\n\n> For this exercise use the quarterly number of arrivals to Australia from New Zealand, 1981 Q1 -- 2012 Q3, from data set `aus_arrivals`.\n\n>   a. Make a time plot of your data and describe the main features of the series.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnzarrivals <- aus_arrivals |> filter(Origin == \"NZ\")\nnzarrivals |> autoplot(Arrivals / 1e3) + labs(y = \"Thousands of people\")\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n\n- The data has an upward trend.\n- The data has a seasonal pattern which increases in size approximately proportionally to the average number of people who arrive per year. Therefore, the data has multiplicative seasonality.\n\n>   b. Create a training set that withholds the last two years of available data. Forecast the test set using an appropriate model for Holt-Winters’ multiplicative method.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnz_tr <- nzarrivals |>\n  slice(1:(n() - 8))\nnz_tr |>\n  model(ETS(Arrivals ~ error(\"M\") + trend(\"A\") + season(\"M\"))) |>\n  forecast(h = \"2 years\") |>\n  autoplot() +\n  autolayer(nzarrivals, Arrivals)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\n\n>   c. Why is multiplicative seasonality necessary here?\n\n- The multiplicative seasonality is important in this example because the seasonal pattern increases in size proportionally to the level of the series.\n- The behaviour of the seasonal pattern will be captured and projected in a model with multiplicative seasonality.\n\n>   d. Forecast the two-year test set using each of the following methods:\n>       i) an ETS model;\n>       ii) an additive ETS model applied to a log transformed series;\n>       iii) a seasonal naïve method;\n>       iv) an STL decomposition applied to the log transformed data followed by an ETS model applied to the seasonally adjusted (transformed) data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc <- nz_tr |>\n  model(\n    ets = ETS(Arrivals),\n    log_ets = ETS(log(Arrivals)),\n    snaive = SNAIVE(Arrivals),\n    stl = decomposition_model(STL(log(Arrivals)), ETS(season_adjust))\n  ) |>\n  forecast(h = \"2 years\")\n\nfc |>\n  autoplot(level = NULL) +\n  autolayer(filter(nzarrivals, year(Quarter) > 2000), Arrivals)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |>\n  autoplot(level = NULL) +\n  autolayer(nzarrivals, Arrivals)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n\n\n\n>   e. Which method gives the best forecasts? Does it pass the residual tests?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  accuracy(nzarrivals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 11\n  .model  Origin .type      ME   RMSE    MAE    MPE  MAPE  MASE RMSSE    ACF1\n  <chr>   <chr>  <chr>   <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 ets     NZ     Test   -3495. 14913. 11421. -0.964  3.78 0.768 0.771 -0.0260\n2 log_ets NZ     Test    2467. 13342. 11904.  1.03   4.03 0.800 0.689 -0.0786\n3 snaive  NZ     Test    9709. 18051. 17156.  3.44   5.80 1.15  0.933 -0.239 \n4 stl     NZ     Test  -12535. 22723. 16172. -4.02   5.23 1.09  1.17   0.109 \n```\n\n\n:::\n:::\n\n\n\n\n- The best method is the ETS model on the logged data (based on RMSE), and it passes the residuals tests.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_ets <- nz_tr |>\n  model(ETS(log(Arrivals)))\nlog_ets |> gg_tsresiduals()\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n\n```{.r .cell-code}\naugment(log_ets) |>\n  features(.innov, ljung_box, lag = 12)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  Origin .model             lb_stat lb_pvalue\n  <chr>  <chr>                <dbl>     <dbl>\n1 NZ     ETS(log(Arrivals))    11.0     0.530\n```\n\n\n:::\n:::\n\n\n\n\n>   f. Compare the same four methods using time series cross-validation instead of using a training and test set. Do you come to the same conclusions?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnz_cv <- nzarrivals |>\n  slice(1:(n() - 3)) |>\n  stretch_tsibble(.init = 36, .step = 3)\n\nnz_cv |>\n  model(\n    ets = ETS(Arrivals),\n    log_ets = ETS(log(Arrivals)),\n    snaive = SNAIVE(Arrivals),\n    stl = decomposition_model(STL(log(Arrivals)), ETS(season_adjust))\n  ) |>\n  forecast(h = 3) |>\n  accuracy(nzarrivals)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 11\n  .model  Origin .type    ME   RMSE    MAE   MPE  MAPE  MASE RMSSE  ACF1\n  <chr>   <chr>  <chr> <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1 ets     NZ     Test  4627. 15327. 11799.  2.23  6.45 0.793 0.797 0.283\n2 log_ets NZ     Test  4388. 15047. 11566.  1.99  6.36 0.778 0.782 0.268\n3 snaive  NZ     Test  8244. 18768. 14422.  3.83  7.76 0.970 0.976 0.566\n4 stl     NZ     Test  4252. 15618. 11873.  2.04  6.25 0.798 0.812 0.244\n```\n\n\n:::\n:::\n\n\n\n\n- An initial fold size (`.init`) of 36 has been selected to ensure that sufficient data is available to make reasonable forecasts.\n- A step size of 3 (and forecast horizon of 3) has been used to reduce the computation time.\n- The ETS model on the log data still appears best (based on 3-step ahead forecast RMSE).\n\n# fpp3 8.8, Ex12\n\n>   a. Apply cross-validation techniques to produce 1 year ahead ETS and seasonal naïve forecasts for Portland cement production (from `aus_production`). Use a stretching data window with initial size of 5 years, and increment the window by one observation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncement_cv <- aus_production |>\n  slice(1:(n() - 4)) |>\n  stretch_tsibble(.init = 5 * 4)\n\nfc <- cement_cv |>\n  model(ETS(Cement), SNAIVE(Cement)) |>\n  forecast(h = \"1 year\")\n```\n:::\n\n\n\n\n>   b. Compute the MSE of the resulting $4$-step-ahead errors. Comment on which forecasts are more accurate. Is this what you expected?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc |>\n  group_by(.id, .model) |>\n  mutate(h = row_number()) |>\n  ungroup() |>\n  as_fable(response = \"Cement\", distribution = Cement) |>\n  accuracy(aus_production, by = c(\".model\", \"h\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 × 11\n  .model           h .type      ME  RMSE   MAE    MPE  MAPE  MASE RMSSE     ACF1\n  <chr>        <int> <chr>   <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>\n1 ETS(Cement)      1 Test  -0.0902  82.9  60.1 -0.227  3.95 0.597 0.625 -0.00185\n2 ETS(Cement)      2 Test  -0.653  101.   72.0 -0.325  4.74 0.708 0.756  0.495  \n3 ETS(Cement)      3 Test  -1.71   119.   87.0 -0.492  5.80 0.856 0.894  0.616  \n4 ETS(Cement)      4 Test  -0.729  137.  102.  -0.543  6.65 1.01  1.03   0.699  \n5 SNAIVE(Ceme…     1 Test  30.9    138.  107.   1.97   6.99 1.06  1.04   0.640  \n6 SNAIVE(Ceme…     2 Test  30.0    139.  107.   1.90   6.96 1.05  1.04   0.649  \n7 SNAIVE(Ceme…     3 Test  29.5    139.  107.   1.85   6.95 1.05  1.04   0.651  \n8 SNAIVE(Ceme…     4 Test  30.8    140.  108    1.91   6.99 1.06  1.05   0.637  \n```\n\n\n:::\n:::\n\n\n\n\nThe ETS results are better for all horizons, although getting closer as $h$ increases. With a long series like this, I would expect ETS to do better as it should have no trouble estimating the parameters, and it will include trends if required.\n\n# fpp3 8.8, Ex13\n\n> Compare `ETS()`, `SNAIVE()` and `decomposition_model(STL, ???)` on the following five time series. You might need to use a Box-Cox transformation for the STL decomposition forecasts. Use a test set of three years to decide what gives the best forecasts.\n\n>   * Beer production from `aus_production`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfc <- aus_production |>\n  filter(Quarter < max(Quarter - 11)) |>\n  model(\n    ETS(Beer),\n    SNAIVE(Beer),\n    stlm = decomposition_model(STL(log(Beer)), ETS(season_adjust))\n  ) |>\n  forecast(h = \"3 years\")\nfc |>\n  autoplot(filter_index(aus_production, \"2000 Q1\" ~ .), level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |> accuracy(aus_production)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model       .type     ME  RMSE   MAE      MPE  MAPE  MASE RMSSE  ACF1\n  <chr>        <chr>  <dbl> <dbl> <dbl>    <dbl> <dbl> <dbl> <dbl> <dbl>\n1 ETS(Beer)    Test   0.127  9.62  8.92  0.00998  2.13 0.563 0.489 0.376\n2 SNAIVE(Beer) Test  -2.92  10.8   9.75 -0.651    2.34 0.616 0.549 0.325\n3 stlm         Test  -2.85   9.87  8.95 -0.719    2.16 0.565 0.502 0.283\n```\n\n\n:::\n:::\n\n\n\n\nETS and STLM do best for this dataset based on the test set performance.\n\n>   * Bricks production from `aus_production`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy_bricks <- aus_production |>\n  filter(!is.na(Bricks))\nfc <- tidy_bricks |>\n  filter(Quarter < max(Quarter - 11)) |>\n  model(\n    ets = ETS(Bricks),\n    snaive = SNAIVE(Bricks),\n    STLM = decomposition_model(STL(log(Bricks)), ETS(season_adjust))\n  ) |>\n  forecast(h = \"3 years\")\nfc |> autoplot(filter_index(aus_production, \"1980 Q1\" ~ .), level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |> accuracy(tidy_bricks)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model .type    ME  RMSE   MAE   MPE  MAPE  MASE RMSSE    ACF1\n  <chr>  <chr> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 STLM   Test   9.71  18.7  14.9 2.29   3.65 0.411 0.378  0.0933\n2 ets    Test   2.27  17.5  13.2 0.474  3.31 0.365 0.354  0.339 \n3 snaive Test  32.6   36.5  32.6 7.85   7.85 0.898 0.739 -0.322 \n```\n\n\n:::\n:::\n\n\n\n\nETS and STLM do best for this dataset based on the test set performance.\n\n>   * Cost of drug subsidies for diabetes (`ATC2 == \"A10\"`) and corticosteroids (`ATC2 == \"H02\"`) from `PBS`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsubsidies <- PBS |>\n  filter(ATC2 %in% c(\"A10\", \"H02\")) |>\n  group_by(ATC2) |>\n  summarise(Cost = sum(Cost))\nsubsidies |>\n  autoplot(vars(Cost)) +\n  facet_grid(vars(ATC2), scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc <- subsidies |>\n  filter(Month < max(Month) - 35) |>\n  model(\n    ETS(Cost),\n    SNAIVE(Cost),\n    STLM = decomposition_model(STL(log(Cost)), ETS(season_adjust))) |>\n  forecast(h = \"3 years\")\nfc |> autoplot(subsidies, level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-34-2.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |>\n  accuracy(subsidies) |>\n  arrange(ATC2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 11\n  .model      ATC2  .type      ME   RMSE    MAE    MPE  MAPE  MASE RMSSE    ACF1\n  <chr>       <chr> <chr>   <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>   <dbl>\n1 ETS(Cost)   A10   Test   1.38e6 2.36e6 1.85e6  5.83   8.74 1.89  2.00   0.177 \n2 SNAIVE(Cos… A10   Test   4.32e6 5.18e6 4.33e6 19.8   19.9  4.42  4.40   0.638 \n3 STLM        A10   Test   3.27e5 1.62e6 1.32e6  0.783  6.73 1.35  1.37  -0.0894\n4 ETS(Cost)   H02   Test   2.70e4 7.65e4 6.45e4  1.99   7.05 1.09  1.07  -0.0990\n5 SNAIVE(Cos… H02   Test  -1.48e4 8.55e4 7.16e4 -1.31   7.88 1.21  1.20   0.0226\n6 STLM        H02   Test   2.24e4 6.83e4 5.63e4  1.61   6.24 0.951 0.959 -0.217 \n```\n\n\n:::\n:::\n\n\n\n\nThe STLM method appears to perform best for both series.\n\n>   * Total food retailing turnover for Australia from `aus_retail`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfood_retail <- aus_retail |>\n  filter(Industry == \"Food retailing\") |>\n  summarise(Turnover = sum(Turnover))\n\nfc <- food_retail |>\n  filter(Month < max(Month) - 35) |>\n  model(\n    ETS(Turnover),\n    SNAIVE(Turnover),\n    STLM = decomposition_model(STL(log(Turnover)), ETS(season_adjust))\n  ) |>\n  forecast(h = \"3 years\")\nfc |>\n  autoplot(filter_index(food_retail, \"2005 Jan\" ~ .), level = NULL)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\nfc |> accuracy(food_retail)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 10\n  .model           .type     ME  RMSE   MAE    MPE  MAPE  MASE RMSSE  ACF1\n  <chr>            <chr>  <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n1 ETS(Turnover)    Test  -151.   194.  170. -1.47   1.65 0.639 0.634 0.109\n2 SNAIVE(Turnover) Test   625.   699.  625.  5.86   5.86 2.35  2.29  0.736\n3 STLM             Test   -12.8  170.  144. -0.189  1.36 0.543 0.554 0.314\n```\n\n\n:::\n:::\n\n\n\n\nThe STLM model does better than other approaches for this dataset.\n\n# fpp3 8.8, Ex14\n\n>   a. Use `ETS()` to select an appropriate model for the following series: total number of trips across Australia using `tourism`, the closing prices for the four stocks in `gafa_stock`, and the lynx series in `pelt`. Does it always give good forecasts?\n\n### tourism\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naus_trips <- tourism |>\n  summarise(Trips = sum(Trips))\naus_trips |>\n  model(ETS(Trips)) |>\n  report()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSeries: Trips \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4495675 \n    beta  = 0.04450178 \n    gamma = 0.0001000075 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]     s[-2]    s[-3]\n 21689.64 -58.46946 -125.8548 -816.3416 -324.5553 1266.752\n\n  sigma^2:  699901.4\n\n     AIC     AICc      BIC \n1436.829 1439.400 1458.267 \n```\n\n\n:::\n\n```{.r .cell-code}\naus_trips |>\n  model(ETS(Trips)) |>\n  forecast() |>\n  autoplot(aus_trips)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n\n\n\nForecasts appear reasonable.\n\n### GAFA stock\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngafa_regular <- gafa_stock |>\n  group_by(Symbol) |>\n  mutate(trading_day = row_number()) |>\n  ungroup() |>\n  as_tsibble(index = trading_day, regular = TRUE)\n\ngafa_stock |> autoplot(Close)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngafa_regular |>\n  model(ETS(Close))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 4 x 2\n# Key:     Symbol [4]\n  Symbol `ETS(Close)`\n  <chr>       <model>\n1 AAPL   <ETS(M,N,N)>\n2 AMZN   <ETS(M,N,N)>\n3 FB     <ETS(M,N,N)>\n4 GOOG   <ETS(M,N,N)>\n```\n\n\n:::\n\n```{.r .cell-code}\ngafa_regular |>\n  model(ETS(Close)) |>\n  forecast(h = 50) |>\n  autoplot(gafa_regular |> group_by_key() |> slice((n() - 100):n()))\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-37-2.png){width=672}\n:::\n:::\n\n\n\n\nForecasts look reasonable for an efficient market.\n\n### Pelt trading records\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npelt |>\n  model(ETS(Lynx))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A mable: 1 x 1\n   `ETS(Lynx)`\n       <model>\n1 <ETS(A,N,N)>\n```\n\n\n:::\n\n```{.r .cell-code}\npelt |>\n  model(ETS(Lynx)) |>\n  forecast(h = 10) |>\n  autoplot(pelt)\n```\n\n::: {.cell-output-display}\n![](ex7-sol_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\n\n- Here the cyclic behaviour of the lynx data is completely lost.\n- ETS models are not designed to handle cyclic data, so there is nothing that can be done to improve this.\n\n>   b. Find an example where it does not work well. Can you figure out why?\n\n- ETS does not work well on cyclic data, as seen in the pelt dataset above.\n\n# fpp3 8.8, Ex15\n\n> Show that the point forecasts from an ETS(M,A,M) model are the same as those obtained using Holt-Winters' multiplicative method.\n\nPoint forecasts from the multiplicative Holt-Winters' method:\n$$\n  \\hat{y}_{t+h|t} = (\\ell_t + hb_t)s_{t+ h - m(k+1)}\n$$\nwhere $k$ is the integer part of $(h-1)/m$.\n\nAn ETS(M,A,M) model is given by\n\\begin{align*}\n  y_t    & = (\\ell_{t-1}+b_{t-1})s_{t-m}(1+\\varepsilon_t) \\\\\n  \\ell_t & = (\\ell_{t-1}+b_{t-1})(1+\\alpha\\varepsilon_t) \\\\\n  b_t    & = b_{t-1} + \\beta(\\ell_{t-1}+b_{t-1})\\varepsilon_t \\\\\n  s_t    & = s_{t-m} (1+\\gamma\\varepsilon_t)\n\\end{align*}\nSo $y_{T+h}$ is given by\n$$\n  y_{T+h} = (\\ell_{T+h-1}+b_{T+h-1})s_{T+h-m}(1+\\varepsilon_{T+h})\n$$\nReplacing $\\varepsilon_{t}$ by zero for $t>T$, and substituting in from the above equations, we obtain\n$$\n  \\hat{y}_{T+h} = (\\ell_{T+h-2}+2b_{T+h-2})s_{T+h-m}\n$$\nRepeating the process a few times leads to\n$$\n  \\hat{y}_{T+h} = (\\ell_{T}+hb_{T})s_{T+h-m}\n$$\n\nNow if $h \\le m$, then we know the value of $s_{T+h-m}$.\n\nIf $m < h \\le 2m$, then we can write\n$$s_{T+h-m} = s_{T+h-2m} (1 + \\gamma\\varepsilon_{T+h-m})$$\nand replace $\\varepsilon_{T+h-m}$ by 0\n\nIf $2m < h \\le 3m$, then we can write\n$$s_{T+h-m} = s_{T+h-3m} (1 + \\gamma\\varepsilon_{T+h-m})(1+\\gamma\\varepsilon_{T+h-2m})$$\nand replace both $\\varepsilon_{T+h-m}$ and $\\varepsilon_{T+h-2m}$ by 0\n\netc.\n\nSo we can replace $s_{T+h-m}$ by $s_{T+h - m(k+1)}$ where $k$ is the integer part of $(k-1)/m$.\n\nThus\n$$\n  \\hat{y}_{T+h|T} = (\\ell_{T}+hb_{T})s_{T+h -m(k+1)}\n$$\nas required.\n",
    "supporting": [
      "ex7-sol_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}