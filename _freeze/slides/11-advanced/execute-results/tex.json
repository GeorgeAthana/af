{
  "hash": "804aa57b854b753e0c4d12fc29b38a7e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"ETC3550/ETC5550 Applied&nbsp;forecasting\"\nauthor: \"Ch11. Advanced methods\"\ninstitute: \"OTexts.org/fpp3/\"\npdf-engine: pdflatex\nfig-width: 7.5\nfig-height: 3.5\nformat:\n  beamer:\n    theme: monash\n    aspectratio: 169\n    fontsize: 14pt\n    section-titles: false\n    knitr:\n      opts_chunk:\n        dev: \"cairo_pdf\"\ninclude-in-header: header.tex\nexecute:\n  echo: false\n  message: false\n  warning: false\n---\n\n\n\n\n# Complex seasonality\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nus_gasoline |> autoplot(Barrels) +\n  labs(\n    x = \"Year\", y = \"Thousands of barrels per day\",\n    title = \"Weekly US finished motor gasoline products\"\n  )\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/gasolinedata-1.pdf)\n:::\n:::\n\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalls <- read_tsv(\"http://robjhyndman.com/data/callcenter.txt\") |>\n  rename(time = `...1`) |>\n  pivot_longer(-time, names_to = \"date\", values_to = \"volume\") |>\n  mutate(\n    date = as.Date(date, format = \"%d/%m/%Y\"),\n    datetime = as_datetime(date) + time\n  ) |>\n  as_tsibble(index = datetime)\ncalls |>\n  fill_gaps() |>\n  autoplot(volume) +\n  labs(\n    x = \"Weeks\", y = \"Call volume\",\n    title = \"5 minute call volume at North American bank\"\n  )\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/callsdata-1.pdf)\n:::\n:::\n\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sugrrants)\ncalls |>\n  filter(yearmonth(date) == yearmonth(\"2003 August\")) |>\n  ggplot(aes(x = time, y = volume)) +\n  geom_line() +\n  facet_calendar(date) +\n  labs(\n    x = \"Weeks\", y = \"Call volume\",\n    title = \"5 minute call volume at North American bank\"\n  )\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/unnamed-chunk-1-1.pdf)\n:::\n:::\n\n\n## Examples\n\n\n::: {.cell}\n\n```{.r .cell-code}\nturkey_elec <- read_csv(\"data/turkey_elec.csv\", col_names = \"Demand\") |>\n  mutate(Date = seq(ymd(\"2000-01-01\"), ymd(\"2008-12-31\"), by = \"day\")) |>\n  as_tsibble(index = Date)\nturkey_elec |> autoplot(Demand) +\n  labs(\n    title = \"Turkish daily electricity demand\",\n    x = \"Year\", y = \"Electricity Demand (GW)\"\n  )\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/turk-1.pdf)\n:::\n:::\n\n\n## TBATS model\n\n\\begin{alertblock}{\\Large TBATS}\n\\textbf{\\Large T}rigonometric terms for seasonality\\\\\n\\textbf{\\Large B}ox-Cox transformations for heterogeneity\\\\\n\\textbf{\\Large A}RMA errors for short-term dynamics\\\\\n\\textbf{\\Large T}rend (possibly damped)\\\\\n\\textbf{\\Large S}easonal (including multiple and\\\\\\hfill non-integer periods)\n\\end{alertblock}\n\n## TBATS model\n\\vspace*{-0.8cm}\\fontsize{13}{14}\\sf\n\n\\begin{align*}\ny_t&= \\text{observation at time $t$}\\\\\ny_t^{(\\omega)} &= \\begin{cases} (y_t^\\omega-1)/\\omega & \\text{if $\\omega\\ne0$};\\\\\n\\log y_t & \\text{if $\\omega=0$}.\\end{cases}\\hspace*{10cm}\\\\\ny_t^{(\\omega)} &= \\ell_{t-1} + \\phi b_{t-1} + \\sum_{i=1}^M s_{t-m_i}^{(i)} + d_t\\\\\n\\ell_t &= \\ell_{t-1} + \\phi b_{t-1} + \\alpha d_t\\\\\nb_t &= (1-\\phi) b + \\phi b_{t-1} + \\beta d_{t}\\\\\nd_t &= \\sum_{i=1}^p \\phi_i d_{t-i} + \\sum_{j=1}^q \\theta_j \\varepsilon_{t-j} + \\varepsilon_t\\\\\ns_t^{(i)} &= \\sum_{j=1}^{k_i} s_{j,t}^{(i)}\n\\end{align*}\n\\begin{textblock}{8}(4.3,7.5)\n\\begin{align*}\ns_{j,t}^{(i)} &= \\phantom{-}s_{j,t-1}^{(i)}\\cos \\lambda_j^{(i)} + s_{j,t-1}^{*(i)}\\sin \\lambda_j^{(i)} + \\gamma_1^{(i)} d_t\\\\\ns_{j,t}^{(i)} &= -s_{j,t-1}^{(i)}\\sin \\lambda_j^{(i)} + s_{j,t-1}^{*(i)}\\cos \\lambda_j^{(i)} + \\gamma_2^{(i)} d_t\n\\end{align*}\n\\end{textblock}\n\n\\only<2->{\\begin{textblock}{5}(7.3,1.8)\\begin{block}{}Box-Cox transformation\\end{block}\\end{textblock}}\n\\only<3->{\\begin{textblock}{5}(7.3,3.4)\\begin{block}{}$M$ seasonal periods\\end{block}\\end{textblock}}\n\\only<4->{\\begin{textblock}{5}(7.3,4.7)\\begin{block}{}global and local trend\\end{block}\\end{textblock}}\n\\only<5->{\\begin{textblock}{5}(7.3,6.0)\\begin{block}{}ARMA error\\end{block}\\end{textblock}}\n\\only<6->{\\begin{textblock}{5}(7.3,7.3)\\begin{block}{}Fourier-like seasonal terms\\end{block}\\end{textblock}}\n\n\\only<7>{\\begin{textblock}{4}(2.5,2.5)\\large\\begin{alertblock}{TBATS}\n\\textbf{T}rigonometric\\\\\n\\textbf{B}ox-Cox\\\\\n\\textbf{A}RMA\\\\\n\\textbf{T}rend\\\\\n\\textbf{S}easonal\n\\end{alertblock}\\end{textblock}}\n\n## Complex seasonality\n\\fontsize{12}{14}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngasoline |>\n  tbats() |>\n  forecast() |>\n  autoplot()\n```\n:::\n\n\n## Complex seasonality\n\\fontsize{12}{14}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncalls |>\n  tbats() |>\n  forecast() |>\n  autoplot()\n```\n:::\n\n\n## Complex seasonality\n\\fontsize{12}{14}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntelec |>\n  tbats() |>\n  forecast() |>\n  autoplot()\n```\n:::\n\n\n## TBATS model\n\n\\begin{alertblock}{\\Large TBATS}\\fontsize{13.5}{19}\\sf\n\\textbf{\\Large T}rigonometric terms for seasonality\\\\\n\\textbf{\\Large B}ox-Cox transformations for heterogeneity\\\\\n\\textbf{\\Large A}RMA errors for short-term dynamics\\\\\n\\textbf{\\Large T}rend (possibly damped)\\\\\n\\textbf{\\Large S}easonal (including multiple and non-integer periods)\n\\end{alertblock}\\vspace*{0.1cm}\\fontsize{13}{15}\\sf\n\n* Handles non-integer seasonality, multiple seasonal periods.\n* Entirely automated\n* Prediction intervals often too wide\n* Very slow on long series\n\n\\vspace*{10cm}\n\n# Vector autoregression\n\n# Neural network models\n\n## Neural network models\n\n\\alert{Simplest version: linear regression}\n\n\\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=2.5cm]\n    \\tikzstyle{every pin edge}=[<-,shorten <=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{hidden neuron}=[neuron, fill=blue!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,...,4}\n    % This is the same as writing \\foreach \\name / \\y in {1/1,2/2,3/3,4/4}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {};\n    % Draw the output layer node\n    \\node[output neuron,pin={[pin edge={->}]right:Output}, right of=I-2, yshift=-0.5cm] (O) {};\n    % Connect every node in the  layer with the output layer\n    \\foreach \\source in {1,...,4}\n        \\path (I-\\source) edge (O);\n    % Annotate the layers\n    \\node[annot] (input) {Input layer};\n    \\node[annot,right of=input] {Output layer};\n\\end{tikzpicture}\\pause\\fontsize{12}{14}\\sf\\vspace*{-0.2cm}\n\n* Coefficients attached to predictors are called \"weights\".\n* Forecasts are obtained by a linear combination of inputs.\n* Weights selected using a \"learning algorithm\" that minimises a \"cost function\".\n\n## Neural network models\n\n\\alert{Nonlinear model with one hidden layer}\n\n\\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=2.5cm]\n    \\tikzstyle{every pin edge}=[<-,shorten <=1pt]\n    \\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]\n    \\tikzstyle{input neuron}=[neuron, fill=green!50];\n    \\tikzstyle{output neuron}=[neuron, fill=red!50];\n    \\tikzstyle{hidden neuron}=[neuron, fill=blue!50];\n    \\tikzstyle{annot} = [text width=4em, text centered]\n    % Draw the input layer nodes\n    \\foreach \\name / \\y in {1,...,4}\n    % This is the same as writing \\foreach \\name / \\y in {1/1,2/2,3/3,4/4}\n        \\node[input neuron, pin=left:Input \\#\\y] (I-\\name) at (0,-\\y) {};\n    % Draw the hidden layer nodes\n    \\foreach \\name / \\y in {1,...,3}\n        \\path[yshift=-.5cm]\n            node[hidden neuron] (H-\\name) at (2.5cm,-\\y cm) {};\n    % Draw the output layer node\n    \\node[output neuron,pin={[pin edge={->}]right:Output}, right of=H-2] (O) {};\n    % Connect every node in the input layer with every node in the\n    % hidden layer.\n    \\foreach \\source in {1,...,4}\n        \\foreach \\dest in {1,...,3}\n            \\path (I-\\source) edge (H-\\dest);\n    % Connect every node in the hidden layer with the output layer\n    \\foreach \\source in {1,...,3}\n        \\path (H-\\source) edge (O);\n    % Annotate the layers\n    \\node[annot,above of=H-2, node distance=2.5cm] (hl) {Hidden layer};\n    \\node[annot,left of=hl] {Input layer};\n    \\node[annot,right of=hl] {Output layer};\n\\end{tikzpicture}\n\\pause\\vspace*{-0.1cm}\\fontsize{13}{13}\\sf\n* A **multilayer feed-forward network** where each layer of nodes receives inputs from the previous layers.\n* Inputs to each node combined using linear combination.\n* Result modified by nonlinear function before being output.\n\n## Neural network models\n\nInputs to hidden neuron $j$ linearly combined:\n$$\nz_j = b_j + \\sum_{i=1}^4 w_{i,j} x_i.\n$$\nModified using nonlinear function such as a sigmoid:\n$$\ns(z) = \\frac{1}{1+e^{-z}},\n$$\nThis tends to reduce the effect of extreme input values, thus making the network somewhat robust to outliers.\n\n## Neural network models\n\n* Weights take random  values to begin with, which are then updated using the observed data.\n* There is an element of randomness in the predictions. So the network is usually trained several times using different random starting points, and the results are averaged.\n* Number of hidden layers, and the number of nodes in each hidden layer, must be specified in advance.\n\n## NNAR models\n\\fontsize{14}{15}\\sf\n\n* Lagged values of the time series can be used as inputs to a neural network.\n* NNAR($p,k$): $p$ lagged inputs and $k$ nodes in the single hidden layer.\n* NNAR($p,0$) model is equivalent to an ARIMA($p,0,0$) model but without stationarity restrictions.\n* Seasonal NNAR($p,P,k$): inputs $(y_{t-1},y_{t-2},\\dots,y_{t-p},y_{t-m},y_{t-2m},y_{t-Pm})$ and $k$ neurons in the hidden layer.\n* NNAR($p,P,0$)$_m$ model is equivalent to an ARIMA($p,0,0$)($P$,0,0)$_m$ model but without stationarity restrictions.\n\n## NNAR models in R\n\\fontsize{14}{16}\\sf\n\n* The `nnetar()` function fits an NNAR($p,P,k$)$_m$ model.\n* If $p$ and $P$ are not specified, they are automatically selected.\n* For non-seasonal time series, default $p=$ optimal number of lags (according to the AIC) for a linear AR($p$) model.\n* For seasonal time series, defaults are $P=1$ and $p$ is chosen from the optimal linear model fitted to the seasonally adjusted data.\n* Default $k=(p+P+1)/2$ (rounded to the nearest integer).\n\n## Sunspots\n\n* Surface of the sun contains magnetic regions that appear as dark spots.\n* These affect the propagation of radio waves and so telecommunication companies like to predict sunspot activity in order to plan for any future difficulties.\n* Sunspots follow a cycle of length between 9 and 14 years.\n\n## Sunspots\n\\fontsize{11}{11}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsunspots <- sunspot.year |> as_tsibble()\nsunspots |> autoplot(value)\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/sunspots-1.pdf)\n:::\n:::\n\n\n## NNAR(9,5) model for sunspots\n\\fontsize{11}{11}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsunspots <- sunspot.year |> as_tsibble()\nfit <- sunspots |> model(NNETAR(value))\nfit |>\n  forecast(h = 20, times = 1) |>\n  autoplot(sunspots, level = NULL)\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/sunspot-nnetar-1.pdf)\n:::\n:::\n\n\n## Prediction intervals by simulation\n\\fontsize{11}{11}\\sf\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit |>\n  forecast(h = 20) |>\n  autoplot(sunspots)\n```\n\n::: {.cell-output-display}\n![](11-advanced_files/figure-beamer/sunspot-nnetar-pi-1.pdf)\n:::\n:::\n\n\n# Bootstrapping and bagging\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}